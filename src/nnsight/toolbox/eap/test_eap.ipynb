{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/share/u/caden/.local/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from nnsight.models.UnifiedTransformer import UnifiedTransformer\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  meta\n",
      "Moving model to device:  meta\n"
     ]
    }
   ],
   "source": [
    "model = UnifiedTransformer(\n",
    "    'gpt2-small',\n",
    "    processing=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "model.local_model.set_use_hook_mlp_in(True)\n",
    "model.local_model.set_use_split_qkv_input(True)\n",
    "model.local_model.set_use_attn_result(True)\n",
    "model.update_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioi_dataset import IOIDataset, format_prompt, make_table\n",
    "\n",
    "N = 25\n",
    "clean_dataset = IOIDataset(\n",
    "    prompt_type='mixed',\n",
    "    N=N,\n",
    "    tokenizer=model.tokenizer,\n",
    "    prepend_bos=False,\n",
    "    seed=1,\n",
    "    device=device\n",
    ")\n",
    "corr_dataset = clean_dataset.gen_flipped_prompts('ABC->XYZ, BAB->XYZ')\n",
    "\n",
    "# make_table(\n",
    "#   colnames = [\"IOI prompt\", \"IOI subj\", \"IOI indirect obj\", \"ABC prompt\"],\n",
    "#   cols = [\n",
    "#     map(format_prompt, clean_dataset.sentences),\n",
    "#     tokenizer.decode(clean_dataset.s_tokenIDs).split(),\n",
    "#     tokenizer.decode(clean_dataset.io_tokenIDs).split(),\n",
    "#     map(format_prompt, clean_dataset.sentences),\n",
    "#   ],\n",
    "#   title = \"Sentences from IOI vs ABC distribution\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean direction: 2.805180311203003, Corrupt direction: 1.5410711765289307\n",
      "Clean metric: 1.0, Corrupt metric: 0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_diff(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    ioi_dataset: IOIDataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    '''\n",
    "        Return average logit difference between correct and incorrect answers\n",
    "    '''\n",
    "    # Get logits for indirect objects\n",
    "    batch_size = logits.size(0)\n",
    "    io_logits = logits[range(batch_size), ioi_dataset.word_idx['end'][:batch_size], ioi_dataset.io_tokenIDs[:batch_size]]\n",
    "    s_logits = logits[range(batch_size), ioi_dataset.word_idx['end'][:batch_size], ioi_dataset.s_tokenIDs[:batch_size]]\n",
    "    # Get logits for subject\n",
    "    logit_diff = io_logits - s_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "\n",
    "with model.invoke(clean_dataset.toks) as clean_invoker:\n",
    "    pass\n",
    "\n",
    "with model.invoke(corr_dataset.toks) as corrupted_invoker:\n",
    "    pass\n",
    "\n",
    "clean_logits = clean_invoker.output\n",
    "corrupt_logits = corrupted_invoker.output\n",
    "\n",
    "clean_logit_diff = ave_logit_diff(clean_logits, clean_dataset).item()\n",
    "corrupt_logit_diff = ave_logit_diff(corrupt_logits, corr_dataset).item()\n",
    "\n",
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    ioi_dataset: IOIDataset = clean_dataset\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_diff(logits, ioi_dataset)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "def negative_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -ioi_metric(logits)\n",
    "    \n",
    "# Get clean and corrupt logit differences\n",
    "with t.no_grad():\n",
    "    clean_metric = ioi_metric(clean_logits, corrupt_logit_diff, clean_logit_diff, clean_dataset)\n",
    "    corrupt_metric = ioi_metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, corr_dataset)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eap_graph\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(eap_graph)\n",
    "\n",
    "graph = eap_graph.EAP(model.config, components=[\"head\", \"mlp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\n",
    "    model,\n",
    "    clean_dataset.toks,\n",
    "    corr_dataset.toks,\n",
    "    batch_size=25,\n",
    "    metric=ioi_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.9.9 -> [-0.027] -> head.11.10.q\n",
      "head.10.7 -> [0.026] -> head.11.10.q\n",
      "head.5.5 -> [0.019] -> head.8.6.v\n",
      "head.9.9 -> [-0.018] -> head.10.7.q\n",
      "head.5.5 -> [-0.017] -> mlp.5\n",
      "mlp.0 -> [-0.017] -> head.7.9.k\n",
      "mlp.0 -> [0.017] -> mlp.5\n",
      "mlp.0 -> [0.014] -> head.6.9.q\n",
      "mlp.0 -> [-0.014] -> head.6.9.k\n",
      "head.5.5 -> [-0.013] -> head.6.9.q\n",
      "mlp.0 -> [-0.013] -> mlp.2\n",
      "mlp.0 -> [0.012] -> head.6.5.k\n",
      "head.9.6 -> [-0.012] -> head.11.10.q\n",
      "head.3.0 -> [-0.012] -> mlp.5\n",
      "head.9.6 -> [-0.012] -> head.10.7.q\n",
      "mlp.5 -> [-0.011] -> mlp.6\n",
      "mlp.0 -> [-0.011] -> head.8.10.k\n",
      "mlp.10 -> [0.01] -> head.11.10.k\n",
      "mlp.0 -> [-0.01] -> mlp.1\n",
      "head.4.11 -> [0.01] -> head.6.9.k\n"
     ]
    }
   ],
   "source": [
    "edges = graph.top_edges(n=20, format=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
